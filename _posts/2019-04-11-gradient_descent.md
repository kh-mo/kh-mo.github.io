---
layout: post
title: 오차의 최소화, Gradient Descent
category: Definition
---

딥러닝에서 사용되는 최적화 방법론인 gradient descent에 관한 내용입니다.
이 분야를 공부하는 사람이라면 누구나 다 알고 있는 내용이지만 그래도 한 번 정리하다보니 어렴풋이 알고만 있던 미흡한 부분이 있었습니다.
그와 관련된 해당 내용을 정리했고 누군가 이 포스트를 통해 이해에 도움을 받을 수 있다면 좋겠습니다. 
포스팅 작성에 도움을 받은 그래디언트에 관한 참고 [위키독스](https://wikidocs.net/6998)와 [블로그](https://darkpgmr.tistory.com/133)는 링크를 걸어두었습니다.
  
## what is the gradient?

다변수 함수 $f$가 $n$개 변수로 이루어져 있다면 다음과 같이 표현할 수 있습니다.

$$ f(x_1, x_2, ..., x_n) $$

이 $f$ 함수의 그래디언트는 함수 $f$를 각 변수로 편미분한 값을 원소로 하는 벡터입니다.

$$ \nabla f=(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}) $$

이 벡터는 기울기가 가장 가파른 곳으로의 방향, $f$값이 가장 가파르게 증가하는 방향을 의미합니다.

예를 들어, $f(x, y) = x^2 + y^2 + xy$라고 하면 그래디언트는 $\nabla f=(2x+y, 2y+x)$ 과 같습니다.
임의의 점 $(1, 3)$에서 함수 $f$ 값이 최대로 증가하는 방향은 $(5, 7)$이고 그 기울기는 $\lVert \sqrt{5^2 + 7^2} \rVert$입니다.

## gradient descent

위에서 그래디언트가 함수값을 최대화시키는 방향을 나타내는 것임을 이야기했습니다.
만약 그래디언트 방향을 반대로 한다면 어떤 함수값을 최소화시키는 방향을 나타낼 것입니다.
이 방법을 활용하여 비용함수를 최소화시키는 최적화 방법론이 gradient descent 방법론입니다.
우선 수식으로 살펴보겠습니다.

$$ x_{n+1} = x_n - \alpha \nabla_x f $$

한 iteration을 돌아 업데이트 되는 $x_{n+1}$은 이전의 $x_n$에서 일정 값을 빼주는 것을 수식에서 확인할 수 있습니다.
$\nabla_x f$는 함수 $f$를 변수 $x$로 편미분한 값입니다.
위의 예제에서 보면 그 값은 임의의 점 $(1, 3)$에서 5를 가졌습니다.
$\alpha$는 이 그래디언트 값 크기를 조절해서 변수 $x$가 급격하게 변해 optimal point를 지나치는 것을 방지하는 역할을 합니다.
마지막으로 그래디언트가 함수를 최대화하는 방향을 가리키기 때문에 최소화하는 반대 방향을 나타내기 위해 음수로 변형합니다.
만약 gradient ascent 방법론을 수행하고자 한다면 다음과 같은 수식을 활용하면 됩니다.

$$ x_{n+1} = x_n + \alpha \nabla_x f $$

여기까지 어떻게 그래디언트 값을 활용해서 변수값을 업데이트 하는지 확인했습니다.
그러나 무한정 학습하는 것은 아닙니다.
Gradient descent 방법은 점진적으로 해를 찾아가는 iterative한 방식이기 때문에 어느 시점에서 학습을 멈춰야합니다.
또 때로는 예기치 못하게 발산할 가능성도 있습니다.
따라서 다음과 같은 조건을 만족할 경우에만 변수를 업데이트하고 학습을 진행합니다.

$$ L(\theta + \nabla \theta) \leq L(\theta) $$

업데이트 된 변수를 기반으로 구한 로스함수 $L$이 업데이트 되기 이전의 변수로 구한 로스함수보다 작거나 같을 경우에만 변수를 업데이트 한다는 조건입니다.
이 조건값이 만족되는 동안 학습이 진행되며 그동안은 gradient descent 방법론이 최적해를 찾아가고 있다고 할 수 있습니다.

<script src="https://gist.github.com/kh-mo/fbecdd96c163b895b5123571fe63d8c1.js"></script>

## backpropagation

## stochastic gradient descent

