---
layout: post
title: Weight Uncertainty
category: Non-Category
---

Uncertainty를 추정하는 여러가지 방법론들이 있습니다.
이번 포스트에서는 해당 방법론들 중 weight를 샘플링하여 uncertainty를 추정하는 방법론에 대해서 살펴보겠습니다.
이 포스트는 ICML 2015에 억셉된 논문 **'Weight Uncertainty in Neural Networks'**을 정리한 것입니다.
잘못된 해석이나 이해가 포함될 수 있으니 첨언과 조언은 언제나 환영합니다.
  
## 어떤 문제를 풀고자 하는가?

Neural Network는 고정된 값을 가진 가중치로 네트워크를 초기화하고 back propagation 방법론을 통해 해당 가중치를 적절한 값으로 변경합니다.
이 학습 방법을 통해 신경망은 훌륭한 성능을 보이며 많은 문제를 효과적으로 풀어냈습니다.
아쉬운 점은 '너무 잘 푼다'는 점입니다.
특히 학습데이터를 너무 잘 풀기 때문에 새로운 데이터에 약하지요.
이른바 overfitting이 발생하기 쉽다는 것입니다.
학습데이터가 전체 모집단을 올바르게 반영한다면 크게 문제될 것이 없겠지만 항상 그렇다고 보장할 수는 없습니다.
그래서 다양한 정규화 기법이 등장했지요.
본 페이퍼에서 제안하는 방법론은 **정규화 기법**의 하나입니다.
해당 정규화 방법론을 통해 신경망을 잘 학습함은 물론 unseen data에 대해서도 합리적인 결과를 준다는 점이 본 논문의 포인트입니다.

## 푸는 방법은?

이 문제를 풀기 위해서 본 논문은 **Bayes by Backprop**이라는 방법론을 제안합니다.
네트워크의 가중치를 bayesian inference 방법론을 통해 얻는 것입니다.


## 실험 결과는?

## 기존의 연구 방향은?

