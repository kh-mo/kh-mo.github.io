---
layout: post
title: Weight Uncertainty
category: Non-Category
---

Uncertainty를 추정하는 여러가지 방법론들이 있습니다.
이번 포스트에서는 해당 방법론들 중 weight를 샘플링하여 uncertainty를 추정하는 방법론에 대해서 살펴보겠습니다.
이 포스트는 ICML 2015에 억셉된 논문 **'Weight Uncertainty in Neural Networks'**을 정리한 것입니다.
잘못된 해석이나 이해가 포함될 수 있으니 첨언과 조언은 언제나 환영합니다.
  
## 어떤 문제를 풀고자 하는가?

Neural Network는 고정된 값을 가진 가중치로 네트워크를 초기화하고 back propagation 방법론을 통해 해당 가중치를 적절한 값으로 변경합니다.
이 학습 방법을 통해 신경망은 훌륭한 성능을 보이며 많은 문제를 효과적으로 풀어냈습니다.
아쉬운 점은 '너무 잘 푼다'는 점입니다.
특히 학습데이터를 너무 잘 풀기 때문에 새로운 데이터에 약하지요.
이른바 overfitting이 발생하기 쉽다는 것입니다.
학습데이터가 전체 모집단을 올바르게 반영한다면 크게 문제될 것이 없겠지만 항상 그렇다고 보장할 수는 없습니다.
때문에 우리가 가진 학습 데이터와 이를 학습한 모델은 결국 불확실성, uncertainty가 존재하게 됩니다.
그리고 이 uncertainty는 모델을 얼마나 신뢰할 수 있는지의 문제로도 연결됩니다.
모델의 신뢰성, 그리고 모델 속의 uncertainty.
페이퍼가 풀고자하는 문제는 이것과 관련되어 있습니다.

## 푸는 방법은?

네트워크 속의 uncertainty를 측정하기 위해 본 논문은 **Bayes by Backprop**이라는 방법론을 제안합니다.
이는 네트워크의 가중치의 확률분포를 variational bayesian learning을 통해 측정하는 방법입니다.
네트워크의 가중치의 확률분포가 무엇일까요?
먼저 논문에 실린 아래 그림을 보겠습니다.

[그림 1 - 논문의 그림1]

왼쪽 그림은 가중치가 특정값으로 고정된 신경망이고 오른쪽 그림은 가중치가 특정 분포로 나타난 신경망입니다.
특정 분포에서 샘플링 된  
## 실험 결과는?

## 기존의 연구 방향은?

