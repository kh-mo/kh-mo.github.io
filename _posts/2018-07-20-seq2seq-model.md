---
layout: post
title: 딥러닝 자연어처리 새 트렌드, sequence to sequence 모델
category: Translation
---

## Meta Information
* 제목 : Sequence to Sequence Learning with Neural Networks
* 2014.09 : arXiv submit
* 2014.12  : NIPS

## 연구 동기
이번에 소개할 논문은 시퀀스(sequence)가 있는 데이터를 처리하는 대표적인 딥러닝 모델인 sequence to sequence 모델(이하 seq2seq)입니다.
Deep Neural Network(DNN)가 레이블 된 데이터를 충분히 가지고 있을 경우 여러 AI 어플리케이션에서 높은 성능을 보이지만, 가장 큰 한계점으로 여겨진 부분은 입력과 출력이 고정된 크기의 벡터로 표현되어야 한다는 것입니다.
그러나 언어처럼 매 순간 입력 길이가 달라져 변동성이 큰 데이터는 DNN이 처리하기 매우 까다롭습니다.
즉, DNN을 이용해서 machine translation, speech recognition, question answering과 같은 문제를 푸는 것은 한계가 있습니다.
그렇기 때문에 이 논문에서는 시퀀스가 데이터를 가진 어떤 문제가 주어졌을 때, 이를 학습할 수 있는 방법론을 제안하는 것을 목적으로 했고 그것이 seq2seq 모델입니다.

## 모델 구조
시퀀스가 있는 정보를 처리하기 위해 이 논문이 사용한 아키텍처는 Long Short Term Memory(LSTM)입니다.
LSTM은 기존의 vanilla RNN보다 긴 입력 시퀀스를 잘 처리하고 vanishing gradient 문제도 상대적으로 많이 줄어든 아키텍처입니다.
Seq2Seq 모델은 LSTM 아키텍처 2개를 사용해 하나는 인코더(encoder), 하나는 디코더(decoder)로 사용하는 모델이 되겠습니다.
이런 구성을 취한 이유는 이 논문이 도전했던 문제가 입력과 출력 시퀀스 길이가 다른 machine translation 문제였기 때문입니다.
논문에서 나타난 그림을 보면 좀 더 이해에 도움이 될 것 같습니다.

![](/public/img/seq2seq-model-figure1.JPG "Figure1 of Sequence to Sequence Learning with Neural Networks")

이 그림에서 입력 시퀀스는 A,B,C,\<EOS\>가 되며 출력 시퀀스는 W,X,Y,Z,\<EOS\>가 됩니다.
이는 우리가 어떤 언어를 번역할 때 단어가 1:1로 매핑되어 완벽하게 같은 시퀀스로 입력과 출력이 나타나지 않는 것으로 상상할 수 있습니다.
위의 그림에서 보듯 입력과 출력 시퀀스는 처리해야 할 길이가 다르기 때문에 입력 정보를 처리하는 인코더 LSTM과 출력 정보를 처리하는 디코더 LSTM을 사용합니다.
인코더가 매 타임스탭마다 정보를 하나씩 받아 처리하여 최종적으로 \<EOS\> 정보를 처리하면 LSTM 아키텍처는 입력 정보를 벡터 하나로 표현하게 됩니다.
디코더는 이 벡터를 받아 매 타임스탭마다 출력해야 할 단어를 예측합니다.
그리고 마지막으로 \<EOS\>를 출력하면 예측이 끝나게 됩니다.
이 과정은 한 시퀀스를 다른 시퀀스에 매핑시키는 종단간(end to end) 학습으로 이루어지게 됩니다.

이번엔 수식적으로 정의해 보겠습니다.
Seq2seq 모델을 학습하기 위해서 우리는 목적함수를 다음과 같이 정의할 수 있습니다.

$$p(y_1,...,y_{T'}|x_1,...,x_T)$$

여기서 $(x_1,...,x_T)$는 입력 시퀀스, $(y_1,...,y_{T'})$는 출력 시퀀스이며 T와 T'를 구별하는 것은 입력과 출력 시퀀스 길이가 다름을 나타내기 위해서입니다.
인코더 LSTM은 입력 시퀀스를 받아 고정된 길이의 벡터 v를 만듭니다.
이 벡터 v를 이용해 디코더 LSTM은 체인룰(chain rule)에 따라 $y_t$를 예측하기 시작하고 다음과 같은 수식으로 정의할 수 있습니다.

$$p(y_1,...,y_{T'}|x_1,...,x_T) = \prod_{t=1}^{T'} p(y_t|v,y_1,...,y_{t-1})$$

이 때 $p(y_t|v,y_1,...,y_{t-1})$는 softmax로 계산된 전체 단어의 확률분포를 의미합니다.
가능한 단어 사전 집합 중 특정 단어 하나가 선택될 확률로 이해해도 좋습니다.
디코더에서 출력 시퀀스 단어들을 순차적으로 예측하다 \<EOS\> 토큰을 출력하면 예측이 종료됩니다.

## 성능 개선을 위한 테크닉
본 논문이 seq2seq 모델을 제안함으로써 뉴럴넷 기반 모델이 번역 문제에서 기존 SMT보다 높은 성능을 보일 수 있게 되었습니다.
당연히 이런 모델 구조를 제안한 것이 이 논문의 가장 큰 기여사항(contribution)이지만 추가적으로 성능을 높일 수 있는 몇 가지 테크닉도 함께 소개되었습니다.
지금부터 그 테크닉을 소개하도록 하겠습니다.

### 1. 빔 서치(Beam Search)
Seq2seq 모델을 통해 문장을 번역하면 딱 1개 번역 결과를 얻게 됩니다.
그러나 실제로 번역 과정에서 오류가 생길 수 있고 더 나은 번역 결과가 존재할 수 있습니다.
그렇기 때문에 한 입력 시퀀스가 모델에 들어갔을 때 출력 시퀀스가 1개가 아닌 n개가 나올 수 있는 방법론으로 빔 서치가 사용됩니다.
디코더에서 매 타임스탭마다 출력되는 값은 가장 확률값이 높은 단어 한 개입니다.
하지만 빔 서치는 가장 확률값이 높은 단어 n개를 매 타임스탭마다 선택하게 됩니다.
이 과정을 반복하여 \<EOS\> 토큰이 출력되는 문장 n개가 나타나면 모델 예측이 종료됩니다.

### 2. 입력 문장 역순으로 넣기(Reverse input sentence)
어떤 모델 구조 변화나 알고리즘을 사용하지 않고도 쉽게 성능을 올리는 방법으로 입력 시퀀스를 역순으로 넣는 방법이 있습니다.
예를들어 한 문장이 A,B,C,\<EOS\> 순서로 구성되어 있을 때, 이 입력 시퀀스를 C,B,A,\<EOS\>로 바꾸는 방법을 의미합니다.
이렇게 할 경우 출력 가능한 단어의 경우의 수를 나타내는 perplexity는 낮아지고 번역 성능을 나타내는 BLEU 스코어는 높아지는 경향성이 나타났다고 합니다.

### 3. 학습 기술(training technique)
실험에 사용된 seq2seq 모델의 하이퍼파라미터 설정에 대해 언급하도록 하겠습니다.
이 조건들을 어떻게 설정하는지 따라 모델이 보이는 성능에 차이가 발생하기 때문에 이 설정은 매우 중요한 포인트가 됩니다.
먼저 인코더와 디코더 **LSTM 아키텍처**는 각각 4개씩 쌓은 구조가 되겠습니다.
이 때 **입력과 출력 임베딩 차원**은 1000이고 **LSTM 셀 차원**도 1000으로 동일합니다.
**입력 사전 크기**는 16만, **출력 사전 크기**는 8만으로 지정했습니다.
학습에 사용된 **파라미터 초기화**는 $X\~U(-0.08, 0.08)$로 지정되었습니다.
**배치 사이즈**는 128을 지정했고 여기서 나오는 그래디언트는 128로 나눠 사용했다고 합니다.<br>
여기까지는 변화가 없는 고정된 파라미터 설정이었고 학습 과정에서 값이 변하는 파라미터와 제약 조건은 다음과 같습니다.
**학습률(Learning Rate)**의 경우 초기 설정값은 0.7 이었지만, 5 에폭(epoch)이 지난 후에는 에폭의 반이 지날때마다 학습률을 절반씩 감소시켰고 최종적으로 7.5 에폭만큼 학습시켰다고 합니다.
또한 그래디언트가 굉장히 커지는 exploding gradient 현상이 나타날 수 있기 때문에 그래디언트 범위를 [10, 25] 사이로 제한했습니다.
이 때 사용한 방식은 $s = ||g||_2$로 계산하여 $s > 5$ 일 때 $ g = \frac{5g}{s}$로 지정했습니다.<br>
마지막으로 학습 속도를 높이기 위해서 배치에 들어가는 문장 길이를 최대한 유사하게 뽑는 방법을 사용했습니다.
입력 시퀀스에 들어갈 한 배치 내 128개 문장 중 길이가 긴 문장과 짧은 문장이 섞여 있다면 짧은 문장은 이미 계산이 완료되었어도 긴 문장을 처리하는 동안 시간 낭비가 발생하게 됩니다.
이 손실을 최소화하기 위해 배치 내 문장 길이를 최대한 맞춰 배치를 구성했습니다.

## 데이터 셋
이 논문이 도전했던 문제는 ‘machine translation’입니다.
세상에 존재하는 언어 수만큼 번역 모델을 만들어 낼 수 있겠지만, 여기서 시도한 번역 모델은 영어에서 프랑스어로 번역하는 모델입니다.
이 작업을 수행하기 위해 WMT’14 English to French 데이터 셋을 사용했습니다.
이 데이터 셋에서 약 1200만 문장을 학습에 사용했다 합니다. 이 문장속에는 영어 단어가 3억 400만개, 프랑스어 단어가 3억 4,800만개 정도가 있다고 합니다.
물론 이 모든 단어를 다 사용하는 것이 아니고, 빈도수를 기준으로 단어를 정렬하여 영어 단어 16만개 프랑스 단어 8만개를 사용했습니다.
그 외 단어는 \<UNK\> 토큰으로 대체하여 사용했습니다.

## 결과
본 모델의 번역 성능을 측정하기 위해 사용한 지표는 BLEU 스코어입니다.
많은 NLP 문제에서 이 지표를 바탕으로 성능을 측정하곤 합니다.
아래 표에서 확인할 수 있듯이 seq2seq 모델이 이제 SMT 모델보다 우수한 성능을 보임을 알 수 있습니다.

|  <center>Method</center> |  <center>test BLEU score</center> |
|:--------|:--------:|
|Baseline System(SMT) | <center>33.30</center> |
|Ensemble of 5 reversed LSTMs, beam size 12 | <center>34.81</center> |

논문 원문을 보면 단일 seq2seq 모델이 베이스라인 모델보다 낮은 성능을 보이지만 충분히 뉴럴넷 기반 모델이 통계 기반의 모델과 견줄 수 있음을 확인할 수 있습니다.

## 블로거 의견
포스트를 작성하는 2018년 현재에 이 seq2seq 모델은 대부분 NLP 영역에서 기본 베이스로 사용되는 것 같습니다.
물론 더 발전된 모델이 많이 나타났지만 가장 기초가 되는 부분은 이 모델이라고 생각합니다.<br>
그리고 본 포스트에는 저의 주관적인 판단과 해석으로 작성되었기 때문에 부정확한 부분이 있을 수 있습니다.
자신만의 견해, 정확한 이해를 위해서 논문을 직접 읽어보시는 것을 권유드립니다.
감사합니다.
