---
layout: post
title: 입력 시퀀스를 고려한 번역 모델, Global Attention
category: Translation
---

## Meta Information
* 제목 : Neural Machine Translation by Jointly Learning to Align and Translate
* 2014.09 : arXiv submit
* 2015.05 : ICLR

## 연구 동기
인코더 디코더 기반의 신경망 네트워크가 번역 모델에서 높은 성능을 낼 수 있는 가능성을 보이면서 더 나은 성능을 얻기 위한 여러 연구가 진행되었습니다.
오늘 소개할 논문은 디코더가 출력할 단어를 예측하는 과정에서 가장 관련성이 높은 입력 문장의 부분을 찾는 모델을 제안했습니다.
기본적인 인코더 디코더 모델은 RNN 아키텍처를 사용하여 입력 문장을 순차적으로 인코딩한 후, 고정된 길이의 벡터(fixed-length vector)를 출력합니다.
그리고 디코더는 이 벡터를 받아 번역 디코딩을 시작합니다.
그러나 고정된 길이의 벡터는 입력 문장의 모든 정보를 벡터 하나로 압축해야 하기 때문에 효율적이지 못합니다.
이 문제는 번역해야 하는 문장이 길어질수록 더 심화됩니다.
본 논문이 이를 해결하기 위해 사용한 방법은 디코더가 다음 단어를 예측할 때, 인코더에서 출력되는 모든 단어 정보 중 가장 관련성이 높은 단어를 찾아 사용하는 방법입니다.
이것이 전역 어텐션(global attention) 방법론입니다.

## 모델 구조
기존의 seq2seq 모델에서 사용된 context vector는 인코더의 마지막 hidden state 였습니다.
그러나 이 벡터는 인코더 입력 정보를 한 벡터로 압축해야 하기 때문에 한계가 있습니다.
본 논문에서 제안하는 방식은 인코더의 마지막 hidden state 만 쓰는 것이 아니라 인코더의 모든 hidden state를 사용합니다.



## 성능 개선을 위한 테크닉

## 데이터 셋

## 결과

## 블로거 의견
그리고 본 포스트에는 저의 주관적인 판단과 해석으로 작성되었기 때문에 부정확한 부분이 있을 수 있습니다.
자신만의 견해, 정확한 이해를 위해서 논문을 직접 읽어보시는 것을 권유드립니다.
감사합니다.
